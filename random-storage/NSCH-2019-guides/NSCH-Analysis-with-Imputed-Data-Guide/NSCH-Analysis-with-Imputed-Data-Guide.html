<p><img src="_page_0_Picture_0.jpeg" alt="" title="" /></p>

<p>April 10, 2024</p>

<h1>National Survey of Children's Health</h1>

<p>Analysis with Multiply Imputed Data</p>

<h1>Multiple imputation details and purpose</h1>

<p>In the National Survey of Children's Health (NSCH), missing values are imputed for several demographic variables used in the construction of survey weights. Child sex, race, and Hispanic origin are imputed using hot-deck imputation while Adult 1 education and household size are imputed using sequential regression imputation methods. Total family income is also imputed using sequential regression as an input to the family poverty ratio (FPL). Imputation is useful because it uses observed data to estimate a plausible response for a missing value. Imputation preserves sample size and avoids the bias of only using observed or known values in a "complete-case" analysis, which assumes that data are missing completely at random. In particular, approximately 15% of the annual NSCH sample is missing one or more components of FPL, which vary by other known demographic characteristics (indicating that data is not missing completely at random). Therefore, it would severely limit sample size and bias estimates to only use the known or reported data.</p>

<p>Using the same sequential regression imputation methods, FPL is also multiply imputed and contains six versions or implicates. Multiple imputation creates several plausible responses for a missing value, using other variables in the dataset to adjust the missing response (Allison, 2001; Rubin, 1996; Schaefer and Graham, 2006). These multiple imputations offer a means of accurately incorporating the uncertainty of the imputed values for missing items. More specifically, combining or averaging estimates across all six imputed values will appropriately increase the standard error to account for this uncertainty while only slightly altering the point estimates. Using only a single imputation, particularly with a large amount of missing data as in the case of FPL, incorrectly assumes certainty in the imputation as if there were no missing data at allâ€”and will produce standard errors that are too low and tests of significance that are too high (increased Type 1 error).</p>

<p>The public use file includes all six imputed values for FPL [FPL_I1-FPL_I6].1 This document includes example code to show how to analyze multiply imputed FPL data using SAS, SAS-callable SUDAAN, and Stata. These procedures or commands will appropriately combine or average the point estimates across implicates and increase standard errors so that significance levels are not overstated. The term implicate will be used in this documentation, although other sources may use imputation (StataCorp LP, 2013).</p>

<h2>Analyzing data in a multiple imputation framework</h2>

<p>The NSCH public use file contains the imputed values stored in different variables, one for each of the imputed responses. These variables contain both fully reported and imputed values. Table 1 shows an example dataset, a wide file, with FPL_I1 -- FPL_I6. For the case ID 1, the FPL_I1 -- FPL_I6 are not identical because there was missing data on either income or household count and these values are imputed. For the case ID 2, the poverty ratio variables are identical because there was no missing data. SAS-callable SUDAAN and Stata can accommodate the wide dataset form.</p>

<p>Table 1. Example of a wide dataset with an imputed observation</p>

<p>| ID | SC<em>AGE | FPL</em>I1 | FPL<em>I2 | FPL</em>I3 | FPL<em>I4 | FPL</em>I5 | FPL_I6 |
|----|--------|--------|--------|--------|--------|--------|--------|
| 1  | 10     | 125    | 135    | 100    | 90     | 130    | 115    |
| 2  | 16     | 250    | 250    | 250    | 250    | 250    | 250    |</p>

<p>Table 2 shows how the dataset needs to be re-organized to do analyses using the multiple imputation variables in SAS with 6 stacked rows of complete data for each observation, one for each implicate. In this long dataset, the variable 'Implicate' reflects the implicate number 1 through 6. In SAS, the actual variable will be called '_Imputation_'. SAS-callable SUDAAN and Stata can use the long dataset form but it is a less efficient form of storage that requires more computational resources.</p>

<p>Table 2. Example of a long dataset with an imputed observation</p>

<p>| ID | SC_AGE | FPL | Implicate |  |
|----|--------|-----|-----------|--|
| 1  | 10     | 125 | 1         |  |
| 1  | 10     | 135 | 2         |  |
| 1  | 10     | 100 | 3         |  |
| 1  | 10     | 90  | 4         |  |
| 1  | 10     | 130 | 5         |  |
| 1  | 10     | 115 | 6         |  |
| 2  | 16     | 250 | 1         |  |
| 2  | 16     | 250 | 2         |  |
| 2  | 16     | 250 | 3         |  |
| 2  | 16     | 250 | 4         |  |
| 2  | 16     | 250 | 5         |  |
| 2  | 16     | 250 | 6         |  |</p>

<h2>Example</h2>

<p>This documentation includes example code for analyzing multiply imputed data in SAS, SAS-callable SUDAAN, and Stata using the 2017 file but can be applied to any data year from 2016 forward. The example code estimates the proportion of children in four poverty categories by children with special health care needs status (SC_CSHCN). We first create a variable named 'povcat_i' that reflects family income as a percentage of the federal threshold by family composition (1='&lt;100% FPL', 2='100%-199% FPL', 3='200%-399% FPL', 4='400%+ FPL').</p>

<h4>How to obtain estimates in SAS:</h4>

<p>In SAS, you will need to reshape data from a wide to long format. This data step is included in the example code. In this step we copy the non-imputed variables (e.g. age) in the dataset along with a single FPL variable and FWC variable, until each respondent has six observations in the dataset, one for each implicate (see Table 2).</p>

<p>Once the data have been reshaped, we can use proc surveymeans to get the mean of the variable poor for each imputed dataset. The proc mianalyze procedure will then combine the estimates by averaging the mean across the implicates and calculate the standard error according to Rubin's formula (Rubin, 1996; SAS Institute, 2009).</p>

<p>```
libname file "&lt;<Replace with file directory>>";
/<strong><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em>*</strong>
In order to use proc mianalyze, we will need to create a long, or stacked,
dataset. Copy the non-imputed variables in the dataset along with a single FPL
variable and FWC variable until each respondent has six observations in the
dataset, one for each implicate.</p>

<hr />

<p>data stacked;
 set file.nsch 2017 topical;
 array fpli{6} fpl<em>i1-fpl</em>i6;
 do Imputation =1 to 6;
   fpl<em>i=fpli{</em>Imputation };
/<strong><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em></strong>
Create a variable named 'povcat i' that reflects family income
as a percentage of the federal threshold by family composition
(1 = 1008FPL', 2 = 1008 - 1998FPL', 3 = 2008 - 3998FPL', 4 = 4008 +FPL').</p>

<hr />

<p>if fpl i &lt; 100 then povcat i = 1;
     if 100 \le fpl i&lt;200 then povcat i = 2;
     if 200 &lt;= fpl i&lt;400 then povcat i = 3;
     if fpl i>=400 then povcat i = 4;
   output;
 end;
run:/<strong><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em>*</strong>
Estimate parameter of interest for each implicate after sorting by
imputation. Use proc surveymeans to get the mean of the variable poor for
each imputed dataset.</p>

<hr />

<p>proc sort data=stacked;
by Imputation ;
run;proc surveyfreq data=stacked;
strata stratum fipsst; * design statements;
cluster hhid;
weight fwc;
by Imputation ; * identify the imputation;
tables sc cshcn<em>povcat i / row cl; * request crosstab with row % and CIs;
ods output crosstabs = min table ; * estimates stored in new dataset mi table;
run;/<strong></em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em></strong>
Sort and combine the implicates by averaging the mean across the
```
National Survey of Children's Health</p>

<p>U.S. Census Bureau</p>

<p>```
implicates using proc mianalyze. This applies Rubin's rules (Rubin, 1996)
to properly inflate standard errors for the imputed cases.</p>

<hr />

<p>proc sort data=mi table;
by sc cshon povcat i;
run:proc mianalyze
data=mi table;
by sc cshcn povcat i; * requests data for each combination of cshcn and
poor;
modeleffects rowpercent ; * combined percentage over all imputations;
stderr rowstderr; * combined standard error over all imputations;
run:
```</p>

<h4>How to obtain estimates in SAS-callable SUDAAN:</h4>

<p>Using SUDAAN, you can leave the data in wide form without re-shaping. A data step is needed to convert the design variables to numeric per SUDAAN requirements and to create the poverty variable. The sorted file can then be analyzed in any procedure using the mi_var statement to identify the implicates. The confidence intervals for SUDAAN crosstab rely on the logit transformation and will be slightly different from the normal or symmetric intervals produced in SAS and Stata.</p>

<p>```
/<strong><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em>*</strong>
SUDAAN can analyze implicate data in two forms (one wide dataset
or separate datasets for each implicate). This example will show
the easier or more efficient option of a single wide dataset.</p>

<hr />

<p>libname file "&lt;<Replace with file directory>>";
data example;
  set file.nsch 2017 topical;
7<strong><em>*</em><em>*</em><em>*</em><em>*</strong>
                               <strong></em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em>*</strong>
Convert the design variables to numeric per SUDAAN
requirements</p>

<hr />

<p>hhidnum = input (hhid, 8.);
fipsstnum = input (fipsst, 8.);if stratum='2A' then stratum='2';
stratumnum = input (stratum, 8.);
/<strong><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em></strong>
                               .<br>Ø§ÛŒÚ© Ø§ÛŒÚ© Ø§ÛŒÚ© Ø§ÛŒÚ© Ø§ÛŒÚ© Ø§ÛŒÚ© Ø§ÛŒÚ© Ø§ÛŒÚ© Ø§ÛŒÚ© Ø§ÛŒÚ© 
Create a variable named 'povcat i' that reflects family income
as a percentage of the federal threshold by family composition
(1 = 100 FPL', 2 = 100 -199 FPL', 3 = 200 -399 FPL', 4 = 400 FPL'.<strong><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em></strong>
                                               <strong></em><em>*</em><em>*</em>**</strong> /
array fpl i{6} fpl i1-fpl i6;
array povcat i{6} povcat i1-povcat i6;
do i=1 to 6;
if fpl i{i}&lt;100 then povcat i{i} = 1;
if 100\overline{5}=fpl i{i}5200 then povcat i{i} = 2;
if 200 &lt;= fpl i{i} &lt;400 then povcat i{i} = 3;
if fpl i(i) >=400 then povcat i(i) = 4;
end;
drop i;
run;
```
National Survey of Children's Health</p>

<p>U.S. Census Bureau</p>

<p>```
/<strong><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em>*</strong>
Sort the data prior to analysis</p>

<hr />

<p>proc sort data=example;
by fipsstnum stratumnum hhidnum;
run:
/<strong><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em><em>*</em>*</strong>
Analyze multiple implicate data using the mi var statement to
identify the implicates. Confidence Intervals reply on the
logit transformation and will be slightly different from the
normal or symmetric intervals produced in SAS and Stata.</p>

<hr />

<p>proc crosstab data=example design=wr ;
nest fipsstnum stratumnum hhidnum / psulev=3; * design statements;
weight fwc;
mi var povcat il-povcat i6; * identifies implicates, called by first
variable listed in remainder of code;
class sc cshcn povcat i1;
table sc cshcn*povcat i1; * requests crosstab;
print nsum wsum rowper serow lowrow uprow /style=nchs nsumfmt=f10.0
wsumfmt=f10.0; * requests row percentages;
run:
```</p>

<h4>How to obtain estimates in Stata:</h4>

<p>In Stata, just as you declare the data to be svyset, you declare it to be an MI (multiple imputation) dataset. In order for Stata to recognize that a variable has been imputed, you need to use mi import and register the imputed variables. Here, you will declare the FPL variables to be imputed.</p>

<p>Stata makes a missing flag when it imputes variables based on the '.' responses. These missing values are not available in the Public Use File. The work-around we advise is generating the variable FPL_IO and then setting all values to '.'. Rubin's (1996) formula will calculate the correct variance across implicates regardless of whether the values were imputed or reported.</p>

<p>Once the data have been imported, and mi set, they are ready for analysis. Simply using the 'mi est: svy:' prefix will combine the estimates by averaging across the implicates and calculate the standard error according to Rubin's formula (Rubin, 1996).</p>

<p>local file = "&lt;<Replace with file directory>>" use "`file'\nsch 2017 topical", clear</p>

<p>```
egen statacross=group(fipsst stratum) /* create single cluster variable for svy */
Generate variable FPL IO and set all values to '.' Because
Stata makes a missing flag when it imputes variables based on
the ',' responses.</p>

<hr />

<p>```
gen fpl_i0=.</p>

<p>save "'file'\nsch_2017_topical", replace /* must be saved prior to declaring imputation */</p>

<p>National Survey of Children's Health</p>

<p>U.S. Census Bureau</p>

<p>/************************************** Import data using mi import and register the imputed variables to declare a multiple imputation (MI) dataset. ***************************************</p>

<p>mi import wide, imputed(fpl_i0=fpl_i1-fpl_i6) drop</p>

<p>/************************************** Create a variable named 'povcat_i' that reflects family income as a percentage of the federal threshold by family composition  $(1 = '100$   $FPL', 2 = '100$   $-199$   $FPL', 3 = '200$   $-399$   $FPL', 4 = '400$   $FPL')$ . *********/</p>

<p>mi passive: generate povcat_i=0 /* generate new variable based on imputed fpl */ mi passive: replace povcat_i=2 if fpl_i0>=100&amp;fpl_i0&lt;200 mi passive: replace povcat_i=3 if fpl_i0>=200&amp;fpl_i0&lt;400 mi passive: replace povcat_i=4 if fpl_i0>=400</p>

<p>/************************************** Use mi est: svy: to combine the estimates by averaging across the implicates and calculate the standard error according to Rubin's formula (Rubin, 1996). ***************************************</p>

<p>mi svyset hhid [pweight=fwc], strata(statacross) /* declare survey data */</p>

<p>mi est: svy: proportion povcat i, over(sc cshcn) /* request crosstab of povcat i by sc cshcn */</p>

<h4><strong>References</strong></h4>

<p>Allison, P. D. 2001. Missing Data. Thousand Oaks: Sage Publications</p>

<ul>
<li>Rubin, D.B. 1996. Multiple Imputation After 18+ Years. Journal of the American Statistical Association 91: 473-489.</li>
<li>Schaefer, J.L. and J.W. Graham. 2002. Missing Data: Our View of State of the Art. Psychological Methods 7(2): 147-177.</li>
<li>SAS Institute Inc. 2009. SAS/STAT 9.2 User's Guide, Second Edition. Cary, NC: SAS Institute Inc.</li>
<li>StataCorp, LP. 2013. Stata Multiple-Imputation Reference Manual: Release 13. College Station, TX: Stata Press.</li>
</ul>
